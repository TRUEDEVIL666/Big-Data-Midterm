
\documentclass{article}
\usepackage{amsmath}
\usepackage{graphicx}

\title{PCY Algorithm for Frequent Itemset Mining and Association Rule Generation using PySpark}
\author{[Your Name], [Group Member 2], [Group Member 3], [Lecturer Name]}
\date{April 2025}

\begin{document}

\maketitle

\section{Abstract}
-[This abstract is for the whole of the project, delete this after transfer]-

In the age of big data, the ability to mine and extract valuable information from massive datasets can give the user an unparalleled edge against the competition. Therefore, these requirements made by the lecturer are designed to simulate three of the most fundamental challenges in data mining. Through this series of tasks, we will explore some algorithm implementations and solve different problems as well as explore their trade-offs. Each task involves a different algorithm to explore and implement with their corresponding datasets. Through these tasks, we will gain practical insight and experience in working with these algorithms as well as a better understanding of their pros and cons to be able to cater to each dataset based on its characteristics.

\section{Introduction}
-[This section is for the introduction of the report - delete after transfer.]-

This report is divided into three large sections corresponding to the first three tasks provided by the lecturer. We will explore and present our findings while putting the proposed algorithms into practice.

Task 1 proposes utilizing the A-Priori algorithm in a Hadoop MapReduce program to discover groups of customers shopping on the same date as well as interacting with HDFS to store files. By applying these methods, we will be able to understand how to extract patterns from large datasets locally.

The second task focuses on implementing the PCY algorithm using OOP principles and PySpark DataFrame to identify frequent item pairs and generate association rules from customer purchase data stored in Google Drive. The implementation, while generating association rules, also has to follow object-oriented programming principles inspired by PySpark's FPGrowth class.

In the third task, we will implement and compare the MinHashLSH algorithm and an alternative of our choice - in this case, a manual method of calculating Jaccard distance. Both of these approaches should achieve the same goal of searching for similar pairs of dates where the Jaccard distance is above a predetermined threshold. After that, we will visualize their runtime with their threshold ranging from 0 to 1 with 0.1 increments to gauge their performance and outline some characteristics between both approaches.

Through these implementations, we demonstrate practical applications of data mining techniques with a given dataset. With these findings, we highlight the trade-offs between various aspects across different algorithms within the given time and constraints.

\section{Approach}
Firstly, we will go through the theoretical basis and its possible implementation in the context of our task, after which, we will see the algorithm in action.

\section{Theoretical basis}
The core of the MinHashLSH algorithm is the utilization of two concepts: MinHash signatures and locality-sensitive hashing (LSH) to create an effective algorithm for detecting similar sets. The first concept describes the process of hashing the dataset into more manageable "signatures" for each set. While the latter describes how these "signatures" will be stored to achieve the expected result.

\subsection{Jaccard distance and Shingling}
Before performing any calculations on any data, we must convert the raw data into a distinguishable vector before using any distance calculation between the two sets to check for their similarity. Shingling performs this task by breaking down text data into smaller units to create "shingles" before hashing them into their representation in the form of a binary vector.

The Jaccard distance describes the similarity between two different sets and is represented by the following formula:

$d_J(A,B) = \frac{|A \cup B| - |A \cap B|}{|A \cup B|} = 1 - J(A,B)
$

With the result ranging from 0 to 1, we can determine whether or not the sets in question are related to each other or not for bucket assignment.

\subsection{MinHash Signature}
In order to determine if the pairs are similar or not, we need a way to convert the raw data into usable data for the algorithm to process. In this case, MinHashing is used to perform this task. The binary vector, created through a process of shingling, is converted into a signature vector. Note that if these sets are similar, their signature vectors will also have some similarities, and these can be utilized by the algorithm to sort these sets into their suitable buckets.

\subsection{Locality Sensitive Hashing}
The idea of LSH when dealing with the problem of finding similar pairs or sets is to maximize the probability of collision in a bucket due to the fact that the hash-code for these sets would be indifferent (if these sets are similar) and therefore they should be in the same bucket. We can achieve this by breaking the hash-code down even more into subsequences of hash-code that has a higher chance of being similar, giving us a higher chance of finding similar pairs, improving the effectiveness of the algorithm at solving the problem.

\end{document}
